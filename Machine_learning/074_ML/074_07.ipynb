{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YA3ZVQlDVn4U"
   },
   "source": [
    "### LAB 7  | CE 74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZXmAl9QRBYPm"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "MKCgPc0IBjQn",
    "outputId": "95dbdbf8-d0d9-4ee8-cb44-4e9148761754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download twitter_samples and stopwords dataset\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aUumh05Gvdw7"
   },
   "outputs": [],
   "source": [
    "# function for preprocessing task and set train data\n",
    "def process_tweet(tweet):\n",
    "  stemmer = PorterStemmer()\n",
    "  stopwords_english = stopwords.words('english')\n",
    "  tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "  tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "  tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "  tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "  tokenizer = TweetTokenizer(preserve_case=False,strip_handles=True,reduce_len=True)\n",
    "  tweet_tokens = tokenizer.tokenize(tweet)\n",
    "  tweets_clean = []\n",
    "  for word in tweet_tokens:\n",
    "    if (word not in stopwords_english and word not in string.punctuation):\n",
    "      stem_word = stemmer.stem(word)\n",
    "      tweets_clean.append(stem_word)\n",
    "  return tweets_clean\n",
    "\n",
    "def build_freqs(tweets, ys):\n",
    "  yslist = np.squeeze(ys).tolist()\n",
    "  freqs = {}\n",
    "  for y, tweet in zip(yslist, tweets):\n",
    "    for word in process_tweet(tweet):\n",
    "      pair = (word, y)\n",
    "      if pair in freqs:\n",
    "        freqs[pair]+=1\n",
    "      else:\n",
    "        freqs[pair]=1\n",
    "  return freqs\n",
    "\n",
    "def extract_features(tweet, freqs):\n",
    "  word_list = process_tweet(tweet)\n",
    "  x = np.zeros((1,2), dtype=np.float32)\n",
    "  for word in word_list:\n",
    "    if (word,1) in freqs:\n",
    "      x[0,0]+=freqs[word,1]\n",
    "    if (word,0) in freqs:\n",
    "      x[0,1]+=freqs[word,0]\n",
    "  assert(x.shape==(1,2))\n",
    "  return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zPk-Lo7txGYH",
    "outputId": "acf3dc49-4246-46f2-8ec1-5d1b55ee2235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alway', 'good', 'compani']\n"
     ]
    }
   ],
   "source": [
    "# sample of preprocessed tweet\n",
    "processed_tweet = process_tweet(\"@Amazon is always #good company\")\n",
    "print(processed_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3133k-Ws0AKQ"
   },
   "outputs": [],
   "source": [
    "# Get the positive and negative tweets and create dataset\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "test_pos = all_positive_tweets[3000:]\n",
    "train_pos = all_positive_tweets[:3000]\n",
    "test_neg = all_negative_tweets[3000:]\n",
    "train_neg = all_negative_tweets[:3000]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "train_y = np.append(np.ones((len(train_pos), 1),np.int64), np.zeros((len(train_neg), 1),np.int64), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1),np.int64), np.zeros((len(test_neg), 1),np.int64), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "tcMoName0ZlW",
    "outputId": "6e3e7fa6-6ed5-448d-fac9-52f92eee501d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(freqs) = <class 'dict'>\n",
      "len(freqs) = 9326\n"
     ]
    }
   ],
   "source": [
    "# Get word frequencies for positive and negative sentiment\n",
    "freqs = build_freqs(train_x,train_y)\n",
    "print(\"type(freqs) = \" + str(type(freqs)))\n",
    "print(\"len(freqs) = \" + str(len(freqs.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VktGii1LbKuG"
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "num_classes = 2 # 1 or 0\n",
    "num_features = 2 # positive and negative freqs\n",
    "learning_rate =  0.001\n",
    "training_steps = 1000\n",
    "batch_size = 256\n",
    "display_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "Z1-SaeAi3BKw",
    "outputId": "81a1ca16-434a-4453-ba88-6ad1893a651b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 1 :  [[2276.   47.]]\n",
      "sample 2 :  [[  45. 2822.]]\n"
     ]
    }
   ],
   "source": [
    "# Get the frequencies of positive and negative word for 2 samples\n",
    "sample_1 = extract_features(train_x[0], freqs)\n",
    "print(\"sample 1 : \", sample_1)\n",
    "sample_2 = extract_features(train_x[4010], freqs)\n",
    "print(\"sample 2 : \", sample_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "FmaC7x6X66Xf",
    "outputId": "8f0c5c53-4007-4852-91da-134d1fa09b53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample :  [ 1.0975696  -0.91117305] [1]\n",
      "Test sample :  [ 1.2294407  -0.74473023] [1]\n"
     ]
    }
   ],
   "source": [
    "# Format X_train and X_test\n",
    "X_train = np.zeros((len(train_x),2),dtype=np.float32)\n",
    "X_test = np.zeros((len(test_x),2),dtype=np.float32)\n",
    "for i in range(len(train_x)):\n",
    "  X_train[i,:] = extract_features(train_x[i],freqs)\n",
    "for i in range(len(test_x)):\n",
    "  X_test[i,:] = extract_features(test_x[i],freqs)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "Y_train = train_y\n",
    "Y_test = test_y\n",
    "print(\"Train sample : \",X_train[0],Y_train[0])\n",
    "print(\"Test sample : \",X_test[1500],Y_test[1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6C_3yo6ZTid"
   },
   "outputs": [],
   "source": [
    "# Intialize weight and bias\n",
    "W = tf.Variable(tf.ones([num_features, num_classes]), name=\"weight\")\n",
    "b = tf.Variable(tf.zeros([num_classes]), name=\"bias\")\n",
    "\n",
    "# Use tf.data API to shuffle and batch data.\n",
    "train_data=tf.data.Dataset.from_tensor_slices((X_train,Y_train))\n",
    "train_data=train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ubkj8_LoheXk"
   },
   "outputs": [],
   "source": [
    "# Main function for perform logistic regression\n",
    "def logistic_regression(x,W,b):\n",
    "  return tf.nn.sigmoid(tf.matmul(x,W) + b)\n",
    "\n",
    "def cross_entropy(y_pred,y_true):\n",
    "  y_true = tf.one_hot(y_true, depth=num_classes)\n",
    "  y_pred = tf.clip_by_value(y_pred,1e-9,1.)\n",
    "  return tf.reduce_mean(-tf.reduce_sum(y_true*tf.math.log(y_pred)))\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "  correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "  return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "def run_optimization(x,y):\n",
    "  with tf.GradientTape() as g:\n",
    "    pred = logistic_regression(x,W,b)\n",
    "    loss = cross_entropy(pred,y)\n",
    "  gradients = g.gradient(loss,[W,b])\n",
    "  optimizer = tf.optimizers.SGD(learning_rate)\n",
    "  optimizer.apply_gradients(zip(gradients, [W,b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "zoix-3yp7fmU",
    "outputId": "342e8082-7786-415e-bc52-97874c688dd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 50, loss: 0.118538, accuracy: 0.356598\n",
      "step: 100, loss: 0.059395, accuracy: 0.406250\n",
      "step: 150, loss: 0.074945, accuracy: 0.519531\n",
      "step: 200, loss: 1.319319, accuracy: 0.550781\n",
      "step: 250, loss: 0.029314, accuracy: 0.574219\n",
      "step: 300, loss: 0.029319, accuracy: 0.578125\n",
      "step: 350, loss: 0.757784, accuracy: 0.472656\n",
      "step: 400, loss: 0.044673, accuracy: 0.429688\n",
      "step: 450, loss: 0.046378, accuracy: 0.386719\n",
      "step: 500, loss: 0.572788, accuracy: 0.468750\n",
      "step: 550, loss: 0.099040, accuracy: 0.585938\n",
      "step: 600, loss: 0.027436, accuracy: 0.578125\n",
      "step: 650, loss: 0.027968, accuracy: 0.621094\n",
      "step: 700, loss: 0.037113, accuracy: 0.437500\n",
      "step: 750, loss: 0.058728, accuracy: 0.421875\n",
      "step: 800, loss: 0.113535, accuracy: 0.417969\n",
      "step: 850, loss: 0.499733, accuracy: 0.449615\n",
      "step: 900, loss: 0.033708, accuracy: 0.488281\n",
      "step: 950, loss: 0.389323, accuracy: 0.515503\n",
      "step: 1000, loss: 0.026424, accuracy: 0.625000\n"
     ]
    }
   ],
   "source": [
    "# Train model for given number of step\n",
    "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
    "  run_optimization(batch_x, batch_y)\n",
    "  if step % display_step == 0:\n",
    "        pred = logistic_regression(batch_x,W,b)\n",
    "        loss = cross_entropy(pred, batch_y)\n",
    "        acc = accuracy(pred, batch_y)\n",
    "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "xUH0PfMoG8RU",
    "outputId": "a51f08d8-dbcf-4acd-b3b1-3ad73896f684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight : \n",
      "<tf.Variable 'weight:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[-0.65007293, -1.4276    ],\n",
      "       [-0.7790809 , -1.7711275 ]], dtype=float32)>\n",
      "Bias : \n",
      "<tf.Variable 'bias:0' shape=(2,) dtype=float32, numpy=array([14.012577, 22.61566 ], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "#Final weight\n",
    "print(\"Weight : \")\n",
    "print(W)\n",
    "#Final bias\n",
    "print(\"Bias : \")\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WZVwtd5lLW1w",
    "outputId": "4b40884d-55fd-468b-d735-1e0655bac426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.500000\n"
     ]
    }
   ],
   "source": [
    "pred = logistic_regression(X_test,W,b)\n",
    "print(\"Test accuracy: %f\" % accuracy(pred,Y_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "074_07_01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
